{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62931500-a186-4d86-86ea-a794e0e456ab",
   "metadata": {},
   "source": [
    "#### Explore AI Academy: Classification Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56028e-c11c-430d-aedd-8555f53f89ab",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 35px\">\n",
    "    <font color='Blue'> <b>Analysing News Articles Dataset</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58119b-69e6-4b95-9857-2dc6efc887a1",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "1. Project Overview\n",
    "     1.1 Introduction\n",
    "        1.1.1 Problem Statement\n",
    "     1.2 Objectives\n",
    "2. Importing Packages\n",
    "3. Loading Data\n",
    "4. Data Cleaning\n",
    "5. Exploratory Data Analysis (EDA)\n",
    "6. Feature Engineering\n",
    "7. Modeling\n",
    "8. Model Performance\n",
    "10.Final Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb57f2-d0f4-4e0d-a252-0c00f4ca0853",
   "metadata": {},
   "source": [
    "> <b> Objective of the Project:</b> To create classification models using Python and deploy it as a web application with Streamlit. The aim is to provide you with a hands-on demonstration of applying machine learning techniques to natural language processing tasks. The primary stakeholders for the news classification project for the news outlet could include the editorial team, IT/tech support, management, readers, etc. These groups are interested in improved content categorization, operational efficiency, and enhanced user experience.\n",
    "\n",
    "> <b> Data Source:</b> To create classification models using Python and deploy it as a web application with Streamlit. The aim is to provide you with a hands-on demonstration of applying machine learning techniques to natural language processing tasks.\n",
    "\n",
    "> <b> Importance of the Study:</b> The dataset is comprised of news articles that need to be classified into categories based on their content, including Business, Technology, Sports, Education, and Entertainment. You can find both the train.csv and test.csv datasets here: https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/refs/heads/main/Data/processed/train.csv\n",
    "\n",
    "> <b> Methodology Overview:</b> This end-to-end project encompasses the entire workflow, including data loading, preprocessing, model training, evaluation, and final deployment.\n",
    "\n",
    "> <b> Structure of the Notebook:</b> The notebook will be structured with the following headings data loading, preprocessing, model training, evaluation, and final deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c171a17",
   "metadata": {},
   "source": [
    "## Project Links\n",
    "\n",
    "- Trello Board Link:https://trello.com/b/JCd6rGTF/classification\n",
    "- Presentation Deck:\n",
    "- Streamlit Link:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cab88-5f94-46e0-8d92-1513bb4861f9",
   "metadata": {},
   "source": [
    "- #### 1.1.1 Problem Statement <a class=\"anchor\" id=\"sub_section_1_1_1\"></a>\n",
    "\n",
    " Build news classification model using deep learning teechniques and deploy model for reporters to classify and label news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad031c54-8a0c-4618-8e25-a53368ebdb7e",
   "metadata": {},
   "source": [
    "### 1.2 Objectives <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d57348-7a65-4ba6-9f38-0cc302cd818a",
   "metadata": {},
   "source": [
    "+ To apply exploratory data analysis.\n",
    "+ To implement feature engineering techniques to extract meaningful information.\n",
    "+ To model and assess various supervised machine learning algorithms for the prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebf1419-48bd-45e9-8f6d-a2b7c0ab8273",
   "metadata": {},
   "source": [
    "## 2. Importing Packages <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "This data set was created to list all shows available on Netflix streaming, and analyze the data to find interesting facts. This data was acquired in July 2022 containing data available in the United States.\n",
    "\n",
    "+ For data manipulation and analysis, `Pandas` and `Numpy`.\n",
    "+ For data visualization, `Matplotlib` and `Seaborn`.\n",
    "\n",
    "Three techniques to vectorize the text data:\n",
    "\n",
    "+ Segment text into words, and convert word into a vector\n",
    "+ Segment text into charactors, and transform each chractors into a vector.\n",
    "+ Extract n-grams of words, and transform each n-grams into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28b1eef2-75f9-46dc-b5ea-84b81aeed71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function warnings.filterwarnings(action, message='', category=<class 'Warning'>, module='', lineno=0, append=False)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries for data loading, manipulation and analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displays output inline\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# set plot style\n",
    "sns.set_theme()\n",
    "\n",
    "# Libraries for Handing Errors\n",
    "import warnings\n",
    "warnings.filterwarnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f882227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import joblib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d662c9",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b1af8-550d-4a7a-b239-4ceb8ac4e695",
   "metadata": {},
   "source": [
    "## 3. Loading Data <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b5e0b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBI revises definition of politically-exposed ...</td>\n",
       "      <td>The central bank has also asked chairpersons a...</td>\n",
       "      <td>The Reserve Bank of India (RBI) has changed th...</td>\n",
       "      <td>https://indianexpress.com/article/business/ban...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...</td>\n",
       "      <td>NDTV's consolidated revenue from operations wa...</td>\n",
       "      <td>Broadcaster New Delhi Television Ltd on Monday...</td>\n",
       "      <td>https://indianexpress.com/article/business/com...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akasa Air ‘well capitalised’, can grow much fa...</td>\n",
       "      <td>The initial share sale will be open for public...</td>\n",
       "      <td>Homegrown server maker Netweb Technologies Ind...</td>\n",
       "      <td>https://indianexpress.com/article/business/mar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India’s current account deficit declines sharp...</td>\n",
       "      <td>The current account deficit (CAD) was 3.8 per ...</td>\n",
       "      <td>India’s current account deficit declined sharp...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>States borrowing cost soars to 7.68%, highest ...</td>\n",
       "      <td>The prices shot up reflecting the overall high...</td>\n",
       "      <td>States have been forced to pay through their n...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  RBI revises definition of politically-exposed ...   \n",
       "1  NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...   \n",
       "2  Akasa Air ‘well capitalised’, can grow much fa...   \n",
       "3  India’s current account deficit declines sharp...   \n",
       "4  States borrowing cost soars to 7.68%, highest ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The central bank has also asked chairpersons a...   \n",
       "1  NDTV's consolidated revenue from operations wa...   \n",
       "2  The initial share sale will be open for public...   \n",
       "3  The current account deficit (CAD) was 3.8 per ...   \n",
       "4  The prices shot up reflecting the overall high...   \n",
       "\n",
       "                                             content  \\\n",
       "0  The Reserve Bank of India (RBI) has changed th...   \n",
       "1  Broadcaster New Delhi Television Ltd on Monday...   \n",
       "2  Homegrown server maker Netweb Technologies Ind...   \n",
       "3  India’s current account deficit declined sharp...   \n",
       "4  States have been forced to pay through their n...   \n",
       "\n",
       "                                                 url  category  \n",
       "0  https://indianexpress.com/article/business/ban...  business  \n",
       "1  https://indianexpress.com/article/business/com...  business  \n",
       "2  https://indianexpress.com/article/business/mar...  business  \n",
       "3  https://indianexpress.com/article/business/eco...  business  \n",
       "4  https://indianexpress.com/article/business/eco...  business  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the train dataset\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/refs/heads/main/Data/processed/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2cf2e",
   "metadata": {},
   "source": [
    "### 3.3 Data Pre-Processing  \n",
    "\n",
    "The datasets, namely train.csv and test.csv, are imported, followed by a cleaning and processing phase to convert the data into a format suitable for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/refs/heads/main/Data/processed/train.csv')  \n",
    "test_data = pd.read_csv('https://raw.githubusercontent.com/Jana-Liebenberg/2401PTDS_Classification_Project/refs/heads/main/Data/processed/test.csv')    \n",
    "\n",
    "# Displaying basic information and the first few rows of each dataset \n",
    "train_info = train_data.info()\n",
    "train_head = train_data.head()\n",
    "test_info = test_data.info()\n",
    "test_head = train_data.head()\n",
    "\n",
    "train_info, train_head, test_info, test_head\n",
    "\n",
    "# Drop the 'url' column\n",
    "print('Dropping URL column...')\n",
    "train_data.drop(columns=['url'], inplace=True, errors='ignore')\n",
    "test_data.drop(columns=['url'], inplace=True, errors='ignore')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text columns for the training set\n",
    "train_data['combined_text'] = train_data['headlines'] + ' ' + train_data['description'] + ' ' + train_data['content']\n",
    "\n",
    "# Combine text columns for the test set\n",
    "test_data['combined_text'] = test_data['headlines'] + ' ' + test_data['description'] + ' ' + test_data['content']\n",
    "\n",
    "# Convert all text in the 'combined_text' column to lowercase\n",
    "print('Lowering case...')\n",
    "train_data['combined_text'] = train_data['combined_text'].str.lower()\n",
    "test_data['combined_text'] = test_data['combined_text'].str.lower()\n",
    "\n",
    "# Define a function to remove punctuation and numbers from the 'combined_text' column\n",
    "import string\n",
    "\n",
    "print('Cleaning punctuation...')\n",
    "def remove_punctuation_numbers(post):\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "\n",
    "# Apply the remove_punctuation_numbers function to the 'combined_text' column\n",
    "train_data['combined_text'] = train_data['combined_text'].apply(remove_punctuation_numbers)\n",
    "test_data['combined_text'] = test_data['combined_text'].apply(remove_punctuation_numbers)\n",
    "\n",
    "# Print the cleaned columns\n",
    "print(train_data['combined_text'])\n",
    "print(test_data['combined_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values (if any)\n",
    "train_data = train_data.dropna(subset=['combined_text', 'category'])\n",
    "test_data = test_data.dropna(subset=['combined_text', 'category'])\n",
    "\n",
    "# Extract features (text) and target (category)\n",
    "X_train = train_data['combined_text']\n",
    "y_train = train_data['category']\n",
    "X_test = test_data['combined_text']\n",
    "y_test = test_data['category']\n",
    "\n",
    "# Convert the target variable into numerical labels (if it's categorical)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert text to features using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f121d-06e2-4fe3-809f-3cb7e4852741",
   "metadata": {},
   "source": [
    "### 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e52c0d-9dc1-4dd1-becc-2066211404c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"Null Values in Train Data:\\n\", train_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada42b0-07bc-4780-b2f8-46f24b88ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values or drop rows (depending on your strategy)\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019e3cc-588d-4544-b9bd-a4b933487b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "train_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485d77d2",
   "metadata": {},
   "source": [
    "### 5. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41433d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Combine all text data\n",
    "text_data = ' '.join(test_data['combined_text'].dropna())\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39becd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "word_counts = vectorizer.fit_transform(test_data['combined_text']).toarray()\n",
    "word_freq = dict(zip(vectorizer.get_feature_names_out(), word_counts.sum(axis=0)))\n",
    "sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "words, counts = zip(*sorted_word_freq)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(words), y=list(counts))\n",
    "plt.title('Top 10 Most Frequent Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4202d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot the distribution of categories\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(x='category', data=test_data)\n",
    "plt.title('Distribution of Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ac333-b5f3-4de7-bb1b-d8114c4c45a5",
   "metadata": {},
   "source": [
    "### 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cdb90-14c2-47f5-9d0d-483adef0df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TF-IDF to transform the text data\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304c3f7-2b77-4fa7-9da4-3bc67394b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'Content' column into numerical data\n",
    "X = vectorizer.fit_transform(train_data['Content'])\n",
    "y = train_data['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00598d8a-49e7-4d3a-b148-e4068cd70693",
   "metadata": {},
   "source": [
    "### 7. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31e7bd",
   "metadata": {},
   "source": [
    "### Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_tfidf, y_train_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d1cfe1-f2e7-401d-83d3-ade12b16fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train a Naive Bayes classifier\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf38ce-a84a-4ff5-9b69-47ccecceff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLflow tracking\n",
    "mlflow.set_experiment(\"News_Classification\")\n",
    "with mlflow.start_run():\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb9264-34f3-4be8-873e-1c0a77e10eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb5e94-5aca-49d6-8efe-ff26ab713154",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Log the metrics to MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64230fee-164a-4019-8878-160b96aeb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878ee7f4",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b0a41",
   "metadata": {},
   "source": [
    "##### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ab149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6176d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Logistic Regression\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression - Accuracy:\", accuracy_score(y_test_encoded, y_pred_log_reg))\n",
    "print(classification_report(y_test_encoded, y_pred_log_reg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8856a25",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35720696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train_tfidf, y_train_encoded)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest - Accuracy:\", accuracy_score(y_test_encoded, y_pred_rf))\n",
    "print(classification_report(y_test_encoded, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352012e",
   "metadata": {},
   "source": [
    "## Evaluate Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification reports for all models\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_log_reg))\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test_encoded, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f88d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
